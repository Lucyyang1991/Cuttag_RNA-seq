---
description: Cut&Tag项目规范
globs: ["*.sh", "*.R", "*.md"]
alwaysApply: true
---
每次修改脚本后，同步更新readme文件，包括增加和减少。

中文回答问题。

# 项目规范（Project Rules）

## 1. 项目概述

本项目是一个Cut&Tag测序数据分析流程，主要用于分析转录因子结合位点。项目包含从原始数据质控、比对到峰值检测和功能注释的完整流程。

## 2. 文件命名规则

- 所有Shell脚本以数字前缀命名（如`00_`，`01_`），表示执行顺序
- 分析步骤脚本命名应当简明扼要地反映其功能（如`01_fastqc.sh`）
- R脚本同样遵循数字前缀命名规则
- 配置文件统一放在`config`目录下
- 结果文件命名应包含样本名、分析步骤和日期信息

## 3. 目录结构规范

```
project_root/
├── LBFC20230270/             # 原始数据及处理脚本
│   ├── config/               # 配置文件
│   ├── *_*.sh                # 各步骤处理脚本
│   └── merge.file/           # 整理好的原始文件（重要）
├── local_analysis/           # 本地数据分析
│   ├── peak_calling/         # 峰值检测结果
│   ├── peak_annotation/      # 峰值注释结果
│   ├── visualization/        # 可视化结果
│   └── *_*.R                 # R分析脚本
└── reference_repos/          # 参考基因组和注释文件
```

### 特殊目录说明
- **LBFC20230270/merge.file/**: 此目录包含整理好的原始文件，是数据分析的主要输入来源

## 4. 代码风格指南

### 通用规范
- **最小改动原则**：每次修改代码时应尽可能做最少量的改动，减少引入新错误的风险
- **单一职责**：每次改动应专注于解决单一问题或实现单一功能
- **改动前测试**：在修改代码前，确保了解现有代码的行为和依赖关系
- **改动后验证**：每次改动后，必须进行适当的测试验证，确保功能正常
- **作者署名**：
  - 在每个脚本文件开头注释中明确标注所有作者信息
  - 如果是合作编写（如与AI助手合作），应同时标注人类作者和AI助手
  - 署名格式示例：
    ```bash
    # 作者：张三
    # 合作者：Claude AI Assistant
    # 创建日期：2024-01-20
    # 最后修改：2024-01-21
    ```
  - 对脚本有实质性贡献的所有参与者都应被列入作者名单
  - 建议注明每位作者的具体贡献内容

### Shell脚本规范
- 每个脚本开头应包含注释，说明脚本功能、输入、输出和依赖
- 每个脚本开头必须包含使用方式说明，包括命令行参数解释和使用示例
  ```bash
  # 使用方式：
  # sh 脚本名.sh [参数1] [参数2] ...
  #   参数1    参数1的功能说明
  #   参数2    参数2的功能说明
  # 示例：
  #   sh 脚本名.sh 参数值1             # 示例1说明
  #   sh 脚本名.sh 参数值1 参数值2     # 示例2说明
  ```
- 使用`00_config.sh`集中管理路径和参数
- 使用有意义的变量名称
- 添加适当的日志输出
- 添加错误处理机制
- **脚本自包含原则**：脚本应当实现所有所需功能，包括激活conda环境和退出conda环境，避免依赖外部手动环境管理
- **环境管理**：所有脚本应在开头使用`source activate cuttag`激活环境，必要时在结束时使用`conda deactivate`退出环境
- **自动化优先**：尽量减少人工干预，脚本应能够自动完成从输入到输出的全部过程
- **SGE作业脚本标准格式**：
  - 所有提交到SGE调度系统的脚本应遵循以下格式：
    ```bash
    #!/bin/bash
    #$ -S /bin/bash
    #$ -N job_name        # 作业名
    #$ -cwd               # 使用当前工作目录
    #$ -j y               # 合并标准输出和错误输出
    #$ -o logs/$JOB_NAME.o$JOB_ID  # 输出文件路径
    #$ -l h_vmem=8G       # 申请内存资源
    #$ -pe smp 4          # 申请CPU线程数
    
    # 脚本正文开始
    source activate cuttag
    
    # 脚本逻辑...
    ```
  - 根据任务资源需求调整内存(-l h_vmem)和CPU线程(-pe smp)参数
  - 确保logs目录存在，或在脚本中创建该目录
  - 作业名应能清晰反映脚本功能和处理的样本
- **SGE作业依赖管理**：
  - 对于有依赖关系的作业，使用`-hold_jid`参数指定依赖的作业ID
  - 示例：`#$ -hold_jid previous_job_name`
  - 复杂流程应创建作业依赖链，确保按正确顺序执行
- **依赖检查**：
  - 在脚本开头添加软件依赖检查代码，确保所有必要工具已正确安装
  - 如发现依赖缺失，提供清晰的错误信息并退出脚本
  - 依赖检查代码示例：
    ```bash
    # 检查必要的软件依赖
    check_dependencies() {
        local missing_tools=()
        
        for tool in fastqc cutadapt bowtie2 samtools seqtk; do
            if ! command -v $tool &> /dev/null; then
                missing_tools+=("$tool")
            fi
        done
        
        if [ ${#missing_tools[@]} -ne 0 ]; then
            echo "错误: 以下必要工具未安装或未添加到PATH中:"
            for tool in "${missing_tools[@]}"; do
                echo "  - $tool"
            done
            echo "请确保已激活正确的conda环境(cuttag)，或安装缺失的工具。"
            exit 1
        fi
    }
    
    # 执行依赖检查
    check_dependencies
    ```
  - 应在激活conda环境后立即执行依赖检查
  - 根据脚本功能调整所需检查的具体工具列表
- **功能单一原则**：
  - 每个脚本应专注于完成单一的分析任务
  - 不同的分析步骤应拆分为独立的脚本（如质控、比对、峰值检测等）
  - 避免在一个脚本中混合多个独立的分析任务
  - 示例：
    - ✓ `01_fastqc.sh`：仅执行质控分析
    - ✓ `02_trimming.sh`：仅执行接头去除
    - ✗ `01_fastqc_and_trimming.sh`：不推荐在一个脚本中同时执行质控和接头去除
  - 好处：
    - 提高代码可读性和可维护性
    - 方便调试和错误定位
    - 支持灵活的任务组合和重用
    - 便于并行执行不同任务

### R脚本规范
- 遵循PEP8编码规范
- 函数和变量命名使用snake_case格式
- 添加详细的注释说明代码逻辑
- 使用相对路径引用数据文件
- 使用tidyverse风格进行数据处理

## 5. 工作环境与文件夹用途

### 环境划分
- **服务器环境**：用于执行计算密集型任务，如测序数据比对、峰值检测等
- **本地环境**：用于数据可视化、统计分析和结果整合等

### 运行环境规范
- **严格分离原则**：LBFC20230270目录下的脚本**只能在服务器环境运行**，不得在本地环境执行
- **环境特异性**：服务器脚本依赖特定的计算资源和环境配置，在本地运行可能导致错误或资源耗尽
- **本地脚本限制**：local_analysis目录下的脚本只应在本地环境运行，不应在服务器上执行
- **跨环境调用**：如需在不同环境间传递数据，应使用专门的数据传输脚本，不应直接调用其他环境的脚本

### conda环境配置
- **服务器端环境名称**：`cuttag`
- **环境激活方式**：所有服务器脚本中使用 `source activate cuttag` 激活环境
- **环境主要组件**：
  - 质控工具：fastqc、multiqc、cutadapt等
  - 比对工具：bowtie2、samtools等
  - 峰值检测：macs2、seacr等
  - 文件处理：bedtools、deeptools等
  - 并行工具：GNU parallel
- **环境管理**：
  - 脚本开头统一添加环境激活命令
  - 确保所有依赖工具在环境中正确安装
  - 环境变量统一在配置文件中管理

### 文件夹用途说明
- **LBFC20230270/**: 
  - 包含在本地撰写但在**服务器上运行**的脚本
  - 处理原始测序数据的工作流程
  - 包含配置文件和各步骤的shell脚本
  
- **reference_repos/**: 
  - 存放从外部获取的**参考分析流程**
  - 作为学习和参考用途，不直接参与实际分析
  - 用于比较和改进当前分析流程 

- **local_analysis/**: 
  - 专门用于**本地执行**的分析脚本和结果
  - 包含R脚本进行下游分析、可视化和统计
  - 处理从服务器获取的中间结果文件

## 6. 并行处理规范

> **注意**: 以下并行处理规范**仅适用于LBFC20230270目录下的脚本**，这些脚本在服务器上执行。local_analysis目录下的本地分析脚本不适用此规范。

### 基本原则
- 所有计算密集型任务均应使用SGE调度系统进行并行处理
- 根据任务特性选择合适的并行方式
- 避免在登录节点上直接运行计算密集型任务

### SGE并行实现方式
1. **数组作业方式**
   - 适用于处理多个独立样本的相同任务
   - 使用`#$ -t 1-N`指定作业数组大小，N为样本数量
   - 使用`SGE_TASK_ID`环境变量访问当前任务ID
   - 在脚本中通过任务ID关联到对应样本
   - 示例：
     ```bash
     #!/bin/bash
     #$ -S /bin/bash
     #$ -N fastqc_array
     #$ -cwd
     #$ -j y
     #$ -o logs/fastqc_$TASK_ID.log
     #$ -t 1-24  # 处理24个样本
     
     # 获取样本列表
     source ./config/00_config.sh
     
     # 获取当前任务对应的样本
     current_sample=${SAMPLES[$SGE_TASK_ID-1]}
     
     # 针对当前样本执行处理
     process_sample ${current_sample}
     ```

2. **样本拆分作业方式**
   - 为每个样本创建单独的作业脚本
   - 使用脚本生成器自动创建每个样本的作业脚本
   - 批量提交所有样本的作业
   - 示例：
     ```bash
     #!/bin/bash
     # 作业生成脚本
     
     source ./config/00_config.sh
     
     # 创建作业脚本目录
     mkdir -p job_scripts
     
     # 为每个样本生成作业脚本
     for sample in "${SAMPLES[@]}"; do
         cat > job_scripts/process_${sample}.sh << EOF
     #!/bin/bash
     #$ -S /bin/bash
     #$ -N process_${sample}
     #$ -cwd
     #$ -j y
     #$ -o logs/process_${sample}.log
     #$ -l h_vmem=8G
     #$ -pe smp 4
     
     source activate cuttag
     
     # 处理单个样本的逻辑
     process_sample ${sample}
     EOF
     
         # 提交作业
         qsub job_scripts/process_${sample}.sh
     done
     ```

3. **资源分配策略**
   - 内存分配：根据任务类型调整-l h_vmem参数
     - 小型任务(FastQC)：4-8G
     - 中型任务(比对)：16-32G
     - 大型任务(峰值检测)：32-64G
   - CPU线程：使用-pe smp参数指定
     - 单线程任务：-pe smp 1
     - 多线程任务：-pe smp 4-8
   - 队列选择：根据任务优先级和资源需求选择合适的队列
     - 短时任务：#$ -q short.q
     - 长时任务：#$ -q long.q

4. **作业监控与管理**
   - 使用qstat命令监控作业状态
   - 使用qdel命令取消作业
   - 在脚本中添加状态检查点，记录关键步骤的完成状态
   - 生成作业完成报告，包含运行时间和资源使用情况

5. **作业依赖和工作流**
   - 使用-hold_jid参数建立作业间依赖关系
   - 示例：上游作业完成后触发下游作业
     ```bash
     # 提交第一个作业
     job1_id=$(qsub job1.sh | cut -d ' ' -f 3)
     
     # 提交依赖于第一个作业的第二个作业
     qsub -hold_jid ${job1_id} job2.sh
     ```
   - 复杂工作流可使用脚本生成作业依赖链

### 日志管理
1. **日志结构**
   - 每个作业生成独立的日志文件
   - 日志文件命名格式：`${作业名}.o${作业ID}`
   - 日志应包含时间戳、处理的样本信息和关键步骤状态

2. **日志分析**
   - 开发日志分析脚本，汇总所有样本处理结果
   - 提取关键指标(如比对率、峰数量等)
   - 生成HTML格式的汇总报告

### 错误处理
1. **作业失败检测**
   - 监控作业退出状态
   - 为关键步骤添加状态检查
   - 自动识别并报告失败的作业

2. **失败重试机制**
   - 开发作业重试脚本，针对失败的作业自动重新提交
   - 限制最大重试次数，避免无限循环
   - 记录重试历史和失败原因

### 示例结构
```bash
# 样本处理主脚本(不直接执行处理，仅提交SGE作业)
#!/bin/bash

source ./config/00_config.sh

# 创建日志目录
mkdir -p logs

# 生成作业脚本
for sample in "${SAMPLES[@]}"; do
    job_script="job_scripts/process_${sample}.sh"
    
    # 创建作业脚本
    cat > ${job_script} << EOF
#!/bin/bash
#$ -S /bin/bash
#$ -N process_${sample}
#$ -cwd
#$ -j y
#$ -o logs/process_${sample}.o\$JOB_ID
#$ -l h_vmem=16G
#$ -pe smp 4

source activate cuttag

# 处理单个样本的逻辑
process_single_sample "${sample}"
EOF

    # 提交作业
    echo "提交样本 ${sample} 的处理作业..."
    qsub ${job_script}
done

echo "所有作业已提交，使用 qstat 查看作业状态"
```

## 7. 分析流程规范

### 数据抽样测试
为确保分析流程的稳定性和可靠性，在开始完整数据分析前，应首先进行抽样测试：

1. **抽样脚本**：
   - 使用`00_subsample.sh`进行随机抽样，该脚本应位于`LBFC20230270/`目录
   - 默认抽取每个样本的10,000条reads用于测试
   - 抽样文件保存在专门的测试目录中（如`test_data/`）

2. **测试输出管理**：
   - **严格分离原则**：测试样品和正式样品的输出文件夹必须严格分开
   - **测试目录结构**：
     ```
     test_data/
     ├── fastq/          # 抽样后的测试用fastq文件
     ├── fastqc/         # 测试样品的质控结果
     ├── bam/            # 测试样品的比对结果
     ├── peaks/          # 测试样品的峰值检测结果
     └── logs/           # 测试过程的日志文件
     ```
   - **命名规范**：测试相关的输出目录应统一添加"test_"前缀
   - **清理机制**：每次正式运行前，应清理或归档旧的测试数据

3. **抽样脚本规范**：
   - 必须包含完整的错误处理和日志记录
   - 使用并行处理加速多样本抽样
   - 生成抽样统计报告，记录每个样本的抽样数量和比例
   - 脚本结构示例：
     ```bash
     #!/bin/bash
     
     # 脚本功能：从原始FASTQ文件中随机抽取指定数量的reads用于测试
     # 输入：原始FASTQ文件目录
     # 输出：抽样后的FASTQ文件和统计报告
     # 依赖：seqtk工具
     
     # 激活conda环境
     source activate cuttag
     
     # 日志设置
     LOG_DIR="./logs"
     mkdir -p ${LOG_DIR}
     LOG_FILE="${LOG_DIR}/subsample_$(date +%Y%m%d).log"
     
     # 抽样函数定义
     subsample_fastq() {
         local sample=$1
         # 抽样逻辑
         # 错误处理
         # 结果验证
         # 日志记录
     }
     
     # 导出函数和变量
     export -f subsample_fastq
     
     # 并行执行抽样
     parallel --will-cite -j 5 subsample_fastq ::: ${SAMPLES[@]}
     
     # 生成统计报告
     ```

4. **测试流程**：
   - 在抽样数据上运行完整分析流程
   - 验证每个步骤的输出是否符合预期
   - 记录资源使用情况和运行时间
   - 根据测试结果优化分析参数

5. **测试验证**：
   - 测试完成后，生成测试报告
   - 报告应包含每个步骤的成功/失败状态
   - 记录潜在问题和优化建议
   
6. **注意事项**：
   - 抽样测试应在每次流程更新后重新执行
   - 测试数据应具有代表性，包含所有样本类型
   - 测试结果应与预期结果比较验证

### 服务器端分析流程（LBFC20230270目录）
1. **数据准备**
   - 数据抽样测试（00_subsample.sh）：从原始数据中抽取部分数据进行流程测试
   - 配置文件准备（00_config.sh）：设置全局变量和路径

2. **数据预处理**
   - FastQC质控（01_fastqc.sh）：评估原始数据质量 （可以不做）
     - 使用FastQC工具进行质量评估
     - 分析参数：--noextract（不解压输出文件）、--nogroup（禁用序列组分析）
     - 并行处理：每个样本使用2个CPU线程
     - 输出HTML格式的质量报告
   
   - FastQC统计（01_fastqc_stats.sh）：生成质控统计报告
     - 汇总所有样本的FastQC结果
     - 分析测序质量分布
     - 统计GC含量分布
     - 评估重复序列比例
     - 生成HTML格式的汇总报告
    
   - Cutadapt去接头（02_trimming.sh）：去除测序接头和低质量序列
     - 去除双端测序接头序列
     - 去除polyG序列（常见于Illumina测序）
     - 去除两端低质量碱基（质量值阈值可配置）
     - 去除含N碱基过多的reads
     - 去除过短的reads（最小长度可配置）
     - 并行处理：每个样本使用4个CPU线程
     - 生成详细的修剪统计报告

   - Trimming统计（02_trimming_stats.sh）：生成接头去除统计报告
     - 统计处理前后的reads数量变化
     - 计算接头匹配率和去除比例
     - 分析reads长度分布变化
     - 评估质量值分布改善情况
    
   - Bowtie2比对（03_alignment.sh）：将处理后的序列比对到参考基因组
     - 使用very-sensitive-local模式进行局部比对
     - 过滤参数：
       - 不输出未比对的reads（--no-unal）
       - 不允许不完整配对（--no-mixed --no-discordant）
       - 插入片段大小限制：10-700bp（-I 10 -X 700）
     - BAM文件处理：
      - 一些分析工具（如SEACR）在处理数据时会考虑重复序列的影响，因此在使用这些工具时，去掉重复序列可能会提高结果的质量。
       - 使用samtools进行排序
       - 使用Picard标记重复序列（不删除，仅标记）
       - 过滤低质量比对（MAPQ < 30）# CebolaLab流程推荐
       - 过滤标记的PCR重复、未比对、次级比对等（-F 1804 = 1024+512+256+8+4）# CebolaLab流程推荐
         - 1024: PCR或光学重复序列
         - 512: 未通过质量控制
         - 256: 次级比对
         - 8: 配对read未比对
         - 4: read未比对
     - 并行处理：每个样本使用8个CPU线程
     - 内存需求：16GB/线程

   - 比对统计（03_alignment_stats.sh）：生成详细的比对质量报告
     - 计算总体比对率和唯一比对率
     - 统计重复序列比例
     - 分析比对质量分布
     - 生成染色体覆盖度统计
     - 计算平均测序深度
     - 生成HTML格式的统计报告

3. **下游分析**
   参考 https://github.com/FredHutch/SEACR/ 流程和MACS3流程
   - Bedgraph生成（04_bedgraph_generation.sh）：生成标准化的bedgraph文件
     - BAM文件预处理：
       - 使用samtools sort -n对BAM文件按read name进行排序
       - 排序后的BAM文件保存在bedgraph目录下（*.namesorted.bam）
       - 并行处理：使用与alignment相同的线程数
       - 每线程内存分配：2GB
     - 使用bedtools bamtobed和genomecov进行转换
       - 首先将name-sorted BAM转换为BEDPE格式
       - 过滤条件：同一染色体且片段长度<1000bp
       - 提取片段起始和终止位置生成fragments.bed
       - 使用bedtools genomecov生成bedgraph
     - 不再使用CPM标准化（由SEACR处理标准化）
     - 生成详细的处理metrics
       - 记录原始read pairs数量
       - 记录过滤后的read pairs数量
       - 记录最终的fragments数量
     - 并行处理：每个样本使用4个CPU线程
     - 内存需求：8GB/线程
    
   - SEACR峰值检测（04_peak_calling_seacr.sh）：使用SEACR进行峰值检测
     - 使用IgG样本作为对照进行峰值检测
     - 支持多个IgG对照的并行处理
     - 使用stringent模式进行峰值检测
     - 自动处理样本配对关系
     - 从配置文件读取样本信息和分组信息
     - 使用norm参数进行IgG对照标准化（未使用spike-in时的推荐设置）
     - 生成标准化的peaks文件（.bed格式）
     - 并行处理：每个样本使用4个CPU线程
     - 内存需求：8GB/线程
     
   - MACS3峰值检测（04_peak_calling_macs3.sh）：使用MACS3进行峰值检测
     - 使用IgG样本作为对照进行峰值检测
     - 优化参数适用于Cut&Tag数据
     - 从配置文件读取样本信息和分组信息
     - 生成标准格式的peaks文件（.narrowPeak格式）
     - 经过测试比较，最终选用MACS3作为主要的峰值检测方法
     - 并行处理：每个样本使用4个CPU线程
     - 内存需求：8GB/线程

   - Peak统计（04_peak_calling_stats.sh）：生成详细的peak calling统计报告
     - 统计每个样本的peak数量
     - 分析peak长度分布（最小值、最大值、平均值、中位数）
     - 汇总bedgraph生成统计信息
     - 进行样本间peak重叠分析
     - 生成HTML格式的统计报告
    
   - BAM转BigWig（05_bam_to_bigwig.sh）：转换比对结果为可视化格式
     - 使用deeptools的bamCoverage工具进行转换
     - 支持多种标准化方法（RPKM/CPM等）
     - 可配置参数：
       - binSize：信号分辨率（默认50bp） # 诺唯赞流程
       - normalizeUsing：标准化方法（默认RPKM）
       - smoothLength：信号平滑长度（默认无）
       - extendReads：reads延伸长度（默认无）
     - 生成IGV track列表和统计报告
     - 并行处理：每个样本使用4个CPU线程
     - 内存需求：8GB/线程
    
   - 热图绘制（06_generate_heatmap.sh）：生成TSS区域的信号富集热图
     - 使用deeptools的computeMatrix和plotHeatmap工具
     - 分析TSS上下游区域的信号分布
     - 支持多个样本的比较分析
     - 生成聚类热图和平均信号曲线
    
   - 样本相关性分析（07_sample_correlation.sh）：分析样本间相关性
     - 使用multiBamSummary和plotCorrelation工具
     - 计算样本间的相关系数
     - 生成相关性热图和PCA图
     - 支持多种相关性计算方法（Pearson/Spearman）

4. **其他工具**
   - 数据抽样（00_subsample.sh）：用于快速测试分析流程
     - 从原始数据中抽取指定比例的reads
     - 支持并行处理多个样本
     - 生成抽样统计报告
     - 自动组织测试数据目录结构
   
   - 任务提交（qsub.sh）：用于批量提交分析任务到SGE集群
     - 自动设置作业依赖关系
     - 管理资源分配
     - 监控作业运行状态
     - 处理作业失败和重试

### 本地分析流程（local_analysis目录）
1. **峰值注释**
   - 使用ChIPseeker进行峰值注释
     - 将峰注释到最近的基因和功能区域（启动子、增强子、UTR等）
     - 分析TSS、TES周围的峰分布
     - 提供基因组区域分布的统计分析
   - 生成综合统计报告
     - 注释类型分布饼图
     - 峰与TSS距离分布图
     - 基因长度和覆盖度分析
   - 可视化注释结果
     - 生成美观的注释饼图
     - 与基因组特征的关联热图
     - TF结合位点的基因组分布图
   - 近端启动子区域筛选（07_filter_proximal_promoter_genes.R）
     - 专门筛选距TSS 1kb以内的启动子区域峰值
     - 提取相关联的基因列表
     - 分析不同样本间的基因重叠关系
     - 生成Venn图展示基因集的交集和差集

2. **RNA-seq差异表达分析**
   - 使用edgeR进行配对比较差异分析（edgeR_analysis.R）
     - 使用配对设计矩阵考虑样本间配对关系
     - 提取上调和下调差异表达基因
     - 生成火山图、热图等可视化结果
     - 特别关注一碳代谢相关基因的表达变化

3. **整合分析**
   - 整合ChIP-seq和RNA-seq数据分析
     - 筛选近端启动子区域峰值的基因与差异表达基因的交集
     - 鉴定直接调控的目标基因
   - 功能富集分析（do_enrichment_analysis.R）
     - 使用clusterProfiler分析交集基因的功能富集
     - GO术语富集分析
     - KEGG通路富集分析
     - 生成富集可视化图表
     - 分析关键生物学通路

### 参考依据
1. **诺唯赞流程**（主要参考）
   - 参考文件：`reference_repos/诺唯赞流程/cuttag_cutrun分析流程v2414/`
   - 提供完整的分析步骤和参数设置
   - 包含详细的质控标准

2. **CebolaLab流程**（方法学参考）
   - 参考文件：`reference_repos/CebolaLab_CUTandTAG_ Analysis pipeline for CUT&TAG data.html`
   - 提供标准化的CUT&TAG分析流程
   - 包含详细的实验设计和数据分析方法

3. **nf-core流程**（补充参考）
   - 参考文件：`reference_repos/nf-core_cutandrun/`
   - 提供标准化的分析流程
   - 包含完整的质控指标

4. **Henikoff教程**（方法学参考）
   - 参考文件：`reference_repos/Henikoff_CUTTag_tutorial/`
   - 提供基础分析方法
   - 包含详细的注释和可视化方法 

## 8. AI助手交互规范

### 代码生成流程
在使用AI助手（如Claude）进行代码生成或脚本修改时，应遵循以下流程：

1. **计划先行原则**：
   - AI助手**必须**首先明确说明其计划和即将执行的操作
   - 计划应包含预期修改的文件、修改的内容和修改的目的
   - 对于复杂修改，应提供分步骤的执行计划

2. **用户确认机制**：
   - AI助手提出计划后，**必须**等待用户确认
   - 用户可以接受、拒绝或修改AI助手的计划
   - 未经用户确认，AI助手不应直接生成或修改代码

3. **执行反馈机制**：
   - 执行计划中的每个重要步骤后，应提供简短的进度更新
   - 遇到问题或需要做出与原计划不同的决定时，应暂停并咨询用户

4. **交互示例**：
   ```
   用户：请帮我修改SEACR峰值检测脚本，添加IgG样本作为对照。
   
   AI助手：我计划修改04_peak_calling_seacr.sh脚本，添加以下功能：
   1. 识别并配对IgG对照样本
   2. 修改SEACR命令，使用IgG作为阈值控制
   3. 添加IgG样本路径验证代码
   您同意这个计划吗？
   
   用户：看起来不错，但请确保IgG样本名称可以从配置文件中读取。
   
   AI助手：好的，我将在计划中增加从配置文件读取IgG样本名称的步骤。修改后的计划是：
   1. 从配置文件读取IgG样本名称
   2. 识别并配对IgG对照样本
   3. 修改SEACR命令，使用IgG作为阈值控制
   4. 添加IgG样本路径验证代码
   现在开始修改脚本...
   ```

5. **方案比较**：
   - 涉及多种可能的解决方案时，AI助手应列出各方案的优缺点
   - 给出推荐方案并说明原因，但最终决定权在用户

### 代码审查规范
在AI助手协助进行代码审查时，应：

1. **完整性检查**：评估代码是否实现了所有预期功能
2. **一致性检查**：确保代码风格与项目其他部分保持一致
3. **安全性检查**：识别潜在的错误处理问题和边界情况
4. **效率评估**：提出可能的性能优化建议 

## 9. 结果文件组织规范

### 目录结构
```
project_root/
├── LBFC20230270/                    # 主项目目录
│   ├── scripts/                     # 分析脚本
│   │   ├── config/                  # 配置文件
│   │   └── *.sh                     # 各步骤处理脚本
│   ├── raw_data/                    # 原始数据
│   ├── genome/                      # 基因组参考文件
│   ├── analysis_results/            # 正式分析结果
│   │   ├── 01_fastqc/              # FastQC质控结果
│   │   │   └── raw/                # 原始数据质控
│   │   ├── 02_trimmed/             # 接头去除结果
│   │   ├── 03_alignment/           # 比对结果
│   │   ├── 04_peaks/               # 峰值检测结果
│   │   │   └── individual_peaks/   # 单个样本峰文件
│   │   ├── 05_visualization/       # 可视化结果
│   │   │   ├── heatmaps/          # 热图
│   │   │   └── correlation/        # 相关性分析
│   │   └── logs/                   # 分析日志
│   │       ├── fastqc/             # 质控日志
│   │       ├── trimming/           # 修剪日志
│   │       ├── alignment/          # 比对日志
│   │       └── peak_calling/       # 峰值检测日志
│   └── test_analysis_results/      # 测试分析结果（与analysis_results结构相同）
└── local_analysis/                  # 本地分析结果
    ├── peak_annotation/            # 峰值注释结果
    ├── functional_analysis/        # 功能富集分析
    └── visualization/              # 结果可视化

```

### 文件命名规范
1. **结果文件命名**：
   - 所有结果文件应包含样本名、分析步骤和日期信息
   - 格式：`{样本名}_{分析步骤}_{日期}.{扩展名}`
   - 示例：`H1_trimmed_20240315.fastq.gz`

2. **日志文件命名**：
   - 格式：`{分析步骤}_{样本名}_{时间戳}.log`
   - 示例：`fastqc_H1_20240315_143022.log`

3. **统计文件命名**：
   - 格式：`{分析步骤}_stats_{日期}.{扩展名}`
   - 示例：`alignment_stats_20240315.txt`

### 数据管理规范
1. **临时文件管理**：
   - 临时文件存放在`temp/`目录
   - 脚本完成后自动清理临时文件
   - 保留最近7天的临时文件用于调试

2. **结果文件管理**：
   - 重要结果文件应有备份
   - 定期清理过期的中间文件
   - 保留所有最终结果和关键中间结果

3. **版本控制**：
   - 重要结果文件应标注版本号
   - 记录分析参数和软件版本
   - 保存完整的分析日志

### 数据备份策略
1. **备份范围**：
   - 原始数据
   - 最终分析结果
   - 分析脚本和配置文件
   - 分析日志

2. **备份频率**：
   - 原始数据：项目开始时完整备份
   - 分析结果：每次重要更新后备份
   - 脚本文件：每次修改后备份

3. **备份位置**：
   - 主要备份：项目服务器
   - 次要备份：外部存储设备
   - 重要结果：多地备份

### 质量控制
1. **数据完整性检查**：
   - 定期验证文件完整性
   - 检查关键结果文件的存在性
   - 验证文件格式的正确性

2. **结果一致性检查**：
   - 比较重复样本的结果
   - 验证分析结果的可重复性
   - 检查异常值和离群值

3. **文档维护**：
   - 更新分析记录和日志
   - 记录异常情况和解决方案
   - 维护结果文件清单
